# log file
# set to 'syslog' without inverted commas if you want elliptics to log through syslog
log = /tmp/log/ioserv1.log

# log level
# DNET_LOG_DATA		= 0
# DNET_LOG_ERROR	= 1
# DNET_LOG_INFO		= 2
# DNET_LOG_NOTICE	= 3
# DNET_LOG_DEBUG	= 4
#log_level = 2
log_level = 3

# specifies whether to join storage network
join = 1

# config flags
# bits start from 0, 0 is unused (its actuall above join flag)
# bit 1 - do not request remote route table
# bit 2 - mix states before read operations according to state's weights
# bit 3 - do not checksum data on upload and check it during data read
# bit 4 - do not update metadata at all
# bit 5 - randomize states for read requests
flags = 4

# node will join nodes in this group
group = 1

# list of remote nodes to connect
#
# address:port:family where family is either 2 (AF_INET) or 10 (AF_INET6)
# address can be host name or IP
#
# It is possible to autodiscover remote clusters via multicast
# If you put 'autodiscovery:address:port:family' where address:port:family is valid multicast address,
# elliptics will broadcast information about itself and remote nodes with the same auth cookie will
# receive this information and connect to given node. Multicast TTL equals to 3.
#remote = 1.2.3.4:1025:2 2.3.4.5:2345:2 autodiscovery:224.0.0.5:1025:2
remote = localhost:1027:2

# local address to bind to
# port 0 means random port
#
# one can use reserved word 'hostname' instead of local address (like hostname:1025:2)
# it will use hostname, the same as output of `hostname -f` command, as address
addr = localhost:1026:2

# wait timeout specifies number of seconds to wait for command completion
wait_timeout = 60

# this timeout specifies number of seconds to wait before killing
# unacked transaction
check_timeout = 60

# number of IO threads in processing pool
io_thread_num = 50

# number of IO threads in processing pool dedicated to nonblocking operations
# they are invoked from recursive commands like DNET_CMD_EXEC, when script
# tries to read/write some data using the same id/key as in original exec command
nonblocking_io_thread_num = 16

# number of thread in network processing pool
net_thread_num = 16

# specifies history environment directory
# it will host file with generated IDs
# and server-side execution scripts
history = /tmp/history1

# specifies whether to go into background
daemon = 1

# authentication cookie
# if this string (32 bytes long max) does not match to server nodes,
# new node can not join and serve IO
auth_cookie = qwerty

# Background jobs (replica checks and recovery) IO priorities
# ionice for background operations (disk scheduler should support it)
# class - number from 0 to 3
# 0 - default class
# 1 - realtime class
# 2 - best-effort class
# 3 - idle class
bg_ionice_class = 3
# prio - number from 0 to 7, sets priority inside class
bg_ionice_prio = 0

# IP priorities
# man 7 socket for IP_PRIORITY
# server_net_prio is set for all joined (server) connections
# client_net_prio is set for other connection
# is only turned on when non zero
server_net_prio = 1
client_net_prio = 6

# Size of operation lock hash table
# These locks guard command execution, they are grabbed for allmost all operations
# except recursive (for example when DNET_CMD_EXEC reads or writes data) and some
# maintenance commands like statistics gathering and route table update
# Recovery process also runs without locks grabbed, since this locks operation quite
# for a long period of time, which may interfere with clients IO
oplock_num = 10240

# SRW - server-side scripting section

# srw worker config
# Elliptics uses cocaine engine (https://github.com/organizations/cocaine) for its server-side workers
# srw_config should point to its configuration file, example config lives in tree in example/library_config.json file
# srw_config = /opt/elliptics/library_config.json
srw_config = /home/derikon/projects/proxy/configs/cocaine_config.json

# In-memory cache support
# This is maximum cache size. Cache is managed by LRU algorithm
# Using different IO flags in read/write/remove commands one can use it
# as cache for data, stored on disk (in configured backend),
# or as plain distributed in-memory cache
cache_size = 102400

# anything below this line will be processed
# by backend's parser and will not be able to
# change global configuration

# backend can be 'filesystem', 'blob', 'smack' or 'leveldb'

backend = filesystem

# Number of bits (from the beginning of the object ID) used
# for directory, which hosts given object
directory_bit_number = 8

# Root directory for data objects
root = /tmp/root1

# zero here means 'sync on every write'
# positive number means file writes are never synced
# and metadata is synced every @sync seconds
sync = 0



#backend = blob

# zero here means 'sync on every write'
# positive number means data amd metadata updates
# are synced every @sync seconds
#sync = 0

# eblob objects prefix. System will append .NNN and .NNN.index to new blobs
#data = /tmp/blob/data

# Align all writes to this boundary
#data_block_size = 1024

# blob processing flags (bits start from 0)
# bit 0 - if set, eblob reserves 10% of total space or size of the blob (which is bigger)
# 		By default it is turned off and eblob only reserves size of the blob
# 		This is useful (needed) to be able to run defragmentation
# bit 1 - overwrite commits write - when set, every overwrite operation will commit its size
# 		To turn overwrite-commits mode you must turn on overwrite mode too, i.e. set bit 2 (blob_flags=6 in config file)
# 		as final, otherwise we will just overwrite
# 		Without this bit set it is equivalent to overwrite parts of the file
# 		When this bit is set, it is like overwriting data and truncating file to given offset + size
# bit 2 - turn on overwrite mode - data can be overwritten in place instead
# 		of appending it at the end. This mode is turned on for metadata
# 		writes (column 1), this bit enables it for all other writes too
# bit 3 - do not append checksum footer - this saves 72 bytes per written record.
# 		This also disables checksum.
# bit 4 - do not check whether system has enough space for the new blob
# bit 5 - reserved for internal use, do not set
# bit 6 - use second hashing layer - greatly reduces memory usage for in-memory eblob index (costs some IOPS)
# 		Likely recommended for everyday use
#blob_flags = 1

# Number of threads used to populate data into RAM at startup
#iterate_thread_num = 1

# Maximum blob size. New file will be opened after current one
# grows beyond @blob_size limit
# Supports K, M and G modifiers
#blob_size = 10G

# Maximum number of records in blob.
# When number of records reaches this level,
# blob is closed and sorted index is generated.
# Its meaning is similar to above @blob_size,
# except that it operates on records and not bytes.
records_in_blob = 10000000

# Timeout for defragmentation process to start
# In every time slot eblob will only defragment one blob,
# since system reserves enough space for only one blob
# After next timeout old (already defragmented into copy)
# blob will be closed (this will actually free space) and
# next one will be defragmented.
#
# Defragmentation operation is rather costly (even if nothing
# is going to be copied, defragmentation still checks every index
# to determine number of removed keys)
# It is recommended to set it to hours (it is in seconds) or more
# Default: -1 or none
defrag_timeout = 3600

# Percentage of removed entries (compared to number of all keys in blob)
# needed to start defragmentation. If number of removed keys is less than
# (removed + not removed) * $defrag_percentage / 100 then defragmentation
# process will skip given blob
defrag_percentage = 25

# Maximum size whole eblob can occupy on disk
# This size will account for all columns (data-XXX.* files) and appropriate indexes
# Basically, this is the maximum size eblob data directory can occupy on disk
blob_size_limit = 10G

# Bloom filter parameters
# index_block_size - number of records from index file, which are hashed into one bloom filter
# eblob splits all records from sorted index file into chunks, each chunk has start and finish
# keys only and bloom filter which says whether requested entry can be found in given chunk
# index_block_bloom_length - number of bits per chunk, it should be at least as twice as number
# of records in chunk
#
# Default values:
# index_block_size = 40
# index_block_bloom_length = 128 * 40

# backend = smack
# Smack is a high-performance backend for small-sized compressible data
# It was build with HBase data storage in mind

# Specifies Smack logger, if not set, main logfile is used
# log = /dev/stderr

# Supported compression types:
#   zlib - default zlib compression
#   zlib_best - best zlib compression, uses more CPU, but compresses data slightly better
#   bzip2 - much slower than zlib (2-3 times in small data sets, about 10-30% for large data sets, like hundreds on millions of records)
#   	    produces best compression, about 30% better compression ration than default zlib
#   snappy - google compression algorithm, is comparable in compression ratio with zlib default, but about 2 times faster
#   lz4_fast - LZ4 compression (http://code.google.com/p/lz4/) - fast compression algorithm
#   lz4_high - high compression ratio algorithm
#
# type = zlib

# Sync-to-disk interval in seconds
# sync = 300

# base directory, data files will look like $root/smack.$idx.data $root/smack.$idx.chunk
# root = /opt/elliptics/smack.2

# Each write goes into cache first, when cache reaches this limit, background thread picks blob and write cache to disk as a contiguous chunk
# The larger the value, the better write performance, but since chunk is compressed, read will have to uncompress it first
# So, the larger this cache size is, the larger is on-disk chunk where it is stored, and the slower uncompression and read performance are
# Cache is per blob, maximum number of blobs is specified in $blob_num variable
# This value likely should not be dramatically changed
# cache_size = 1000

# Size of the bloom filter in bytes - helps to quickly find if given record does not exist in the storage
# bloom_size = 1024

# Maximum number of blob files per storage
# Blob stores multiple data chunks and resorts whole content sometimes to ensure data is stored in a sorted order
# For fixed number of total records in the storage, the more blobs we have, the smaller is each blob's size,
# which means smaller resort times and thus faster writes
#
# Number of blobs MUST be sufficiently large so that every single blob could fit your RAM
# It is a good idea to set limit large enough so that several blobs fit memory
# blob_num = 50

# Number of background threads to write cache to disk and resort on-disk chunks
# On a heavily write-loaded systems small number of cache thread will not be able to quickly resort all written data
# It may be a good idea to set cache thread number to number of CPUs or slightly less (if processors are not supposed to be used
# for other tasks)
# cache_thread_num = 1


# backend = leveldb
#
# One may check discussion at http://www.ioremap.net/node/708/
#
# Sync in leveldb differs from what it is in all other backends
#sync = 0
#root = /opt/elliptics/leveldb
#log = /var/log/elliptics/leveldb.log
#cache_size = 100000
#write_buffer_size = 100000
#block_size = 4096
#block_restart_interval = 8
#compression = snappy
